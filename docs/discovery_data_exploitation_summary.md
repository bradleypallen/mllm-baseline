# Discovery Data Exploitation: Complete Experimental History

## Overview
This document summarizes all attempts to leverage the discovery dataset (14,950 queries with LLM responses) to improve LLM ranking performance beyond the baseline models trained only on the development qrels (342 queries, 386,802 relevance judgments).

## Experimental Sequence

### 1. Epistemic Profiling with Discovery Data
**Concept**: Build expertise profiles for each LLM by analyzing their response patterns on discovery queries. Use semantic similarity and response quality metrics to identify which LLMs are experts in which domains.

**Implementation**:
- Created bilateral profiles analyzing LLM response patterns
- Built query-LLM similarity matrices
- Attempted to correlate discovery performance with qrel judgments

**Result**: **Failed - No improvement**
- Correlation between discovery profiles and qrel judgments was weak
- Domain expertise patterns from discovery didn't transfer to dev set performance
- Too much noise in unsupervised discovery signals

### 2. Collaborative Pre-training
**Concept**: Use discovery data to pre-train query and LLM encoders by learning from the interaction patterns between queries and LLM responses, then fine-tune on qrels.

**Implementation**:
- Created `collaborative_pretraining.py` 
- Pre-trained a query encoder using discovery query-response pairs
- Used contrastive learning on discovery interactions

**Result**: **Successful - Enabled Config O**
- Pre-trained query encoder saved as `collaborative_pretrained_model.pth`
- Provided better initialization for downstream models
- Key component of Config O's success

### 3. Response Quality Weak Labeling (Config L)
**Concept**: Automatically assess LLM response quality on discovery queries using heuristics (length, keyword presence, structure) to generate weak relevance labels.

**Implementation**:
- `weak_label_discovery_mpnet.py` using sentence transformers
- Generated 489,685 weak labels from discovery data
- Combined weak labels with original training data

**Result**: **Major Success - 11.7% improvement**
- Config L: 0.4289 ± 0.0401 nDCG@10 (vs 0.3860 baseline)
- Weak labels provided valuable training signal
- Best single use of discovery data

### 4. Enhanced Weak Labeling with Config O Architecture
**Concept**: Combine the successful weak labeling approach with the improved Config O architecture (4-layer LLM tower, multi-head attention).

**Implementation**:
- Config O architecture + weak labeled data
- 876,486 total training examples

**Result**: **New Champion - 16.1% improvement**
- Config O: 0.4482 ± 0.0353 nDCG@10
- Best overall performance achieved
- Synergy between architecture and weak labels

### 5. Confidence-Weighted Weak Labels
**Concept**: Add confidence scores to weak labels based on response quality assessment certainty, allowing the model to weight more reliable labels higher.

**Implementation**:
- Added confidence scores to weak labeling
- Modified loss function to weight by confidence

**Result**: **No improvement**
- Performance similar to uniform weighting
- Added complexity without benefit

### 6. Pseudo-Labeling with Teacher Model (Config R)
**Concept**: Use a trained model (Config O) as a teacher to generate pseudo-labels for discovery data, implementing semi-supervised learning.

**Implementation**:
- Generated 4,000 pseudo-labels from 200 discovery queries
- Combined original + weak labels + pseudo-labels
- 880,486 total training examples

**Result**: **Failed - Degraded performance**
- Config R: 0.4241 ± 0.0557 nDCG@10
- 5.4% worse than Config O (0.4482)
- Pseudo-labels added noise rather than signal

### 7. Discovery Confidence Features
**Concept**: Extract confidence and quality features from discovery responses to use as additional input features for ranking models.

**Implementation**:
- Created confidence features from response analysis
- Added as additional inputs to models

**Result**: **Minimal impact**
- Marginal improvements not statistically significant
- Features too noisy for reliable signal

### 8. Query-Aware Synthetic Data Generation
**Concept**: Generate synthetic training data by analyzing patterns in discovery query-response pairs to create new training examples.

**Implementation**:
- `query_aware_synthetic_generator.py`
- Generated synthetic qrels based on discovery patterns

**Result**: **No improvement**
- Synthetic data didn't capture true relevance patterns
- Generated examples were too artificial

### 9. Massive Synthetic Data Generation
**Concept**: Scale up synthetic data generation to create millions of training examples based on discovery patterns.

**Implementation**:
- `massive_synthetic_data_generator.py`
- Generated 5M+ synthetic examples

**Result**: **Failed - Overfitting**
- Model overfit to synthetic patterns
- Validation performance degraded
- Too much synthetic data overwhelmed real signal

## Summary

### Successful Approaches:
1. **Collaborative Pre-training** - Provided better initialization
2. **Weak Labeling** - 11.7% improvement (Config L)
3. **Weak Labels + Better Architecture** - 16.1% improvement (Config O)

### Failed Approaches:
1. **Epistemic Profiling** - Weak correlation with qrels
2. **Pseudo-labeling** - Degraded performance
3. **Confidence Features** - Too noisy
4. **Synthetic Data Generation** - Artificial patterns didn't transfer
5. **Massive Synthetic Data** - Overfitting

### Key Insights:
- **Simple heuristic weak labeling** was the most effective use of discovery data
- **Pre-training** on discovery helped with initialization but not direct performance
- **Teacher-student approaches** (pseudo-labeling) failed due to distribution mismatch
- **Complex profiling methods** couldn't extract reliable signals from unsupervised data
- The discovery data is most valuable when used with simple, robust methods rather than sophisticated approaches

### Current Best Model:
**Config O**: nDCG@10 = 0.4482 ± 0.0353
- Uses collaborative pre-trained query encoder
- Trains on original + weak labeled data (876K examples)
- 4-layer LLM tower with multi-head attention
- 16.1% improvement over Random Forest baseline